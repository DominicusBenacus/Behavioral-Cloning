{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#common import\n",
    "import random\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "#from resize_nomalize import resize_normalize\n",
    "from generate_samples import generate_samples\n",
    "from generator_fernando import generator_fernando\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import gc\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape of the first row of samples after imread: ['C:\\\\udacity\\\\data\\\\IMG\\\\center_2017_05_01_18_22_08_311.jpg', 'C:\\\\udacity\\\\data\\\\IMG\\\\left_2017_05_01_18_22_08_311.jpg', 'C:\\\\udacity\\\\data\\\\IMG\\\\right_2017_05_01_18_22_08_311.jpg', '0', '1', '0', '30.19031']:\n",
      " shape of the training_samples: ['C:\\\\udacity\\\\data\\\\IMG\\\\center_2017_05_01_20_22_29_100.jpg', 'C:\\\\udacity\\\\data\\\\IMG\\\\left_2017_05_01_20_22_29_100.jpg', 'C:\\\\udacity\\\\data\\\\IMG\\\\right_2017_05_01_20_22_29_100.jpg', '-0.1330472', '1', '0', '30.19023']:\n",
      " shape of the validation_samples: ['C:\\\\udacity\\\\data\\\\IMG\\\\center_2017_05_01_20_22_11_836.jpg', 'C:\\\\udacity\\\\data\\\\IMG\\\\left_2017_05_01_20_22_11_836.jpg', 'C:\\\\udacity\\\\data\\\\IMG\\\\right_2017_05_01_20_22_11_836.jpg', '-0.004291845', '1', '0', '30.20167']:\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================================================\n",
    "# Read in rough balanced data Set\n",
    "# ================================================================================================================\n",
    "samples = []\n",
    "with open('../data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile) # create a reader object to with\n",
    "    for sample in reader:        # got through all lines of the csv File\n",
    "      samples.append(sample)     # append every line to the list samples[]   \n",
    "del(samples[0])                  # delete the header of the csv file\n",
    "\n",
    "# print out the shape of the first line of list samples[]\n",
    "print(\" shape of the first row of samples after imread: {}:\".format(samples[0]))\n",
    "\n",
    "# Split data into training and validation set\n",
    "#sklearn.model_selection.StratifiedShuffleSplit\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "print(\" shape of the training_samples: {}:\".format(train_samples[0]))\n",
    "print(\" shape of the validation_samples: {}:\".format(validation_samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am before call of architecture\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================================================\n",
    "# Model Architectures\n",
    "# The Nvidia architecture like described im here https://arxiv.org/pdf/1604.07316.pdf\n",
    "# ================================================================================================================\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Dense, Activation, Dropout, MaxPooling2D, Flatten, Lambda, ELU\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import models, optimizers, backend\n",
    "from keras.models import load_model\n",
    "\n",
    "print('I am before call of architecture')\n",
    "def architecture():\n",
    "    # put the normalization fucntion inside the model ensure preprocess using Lambda layer\n",
    "    # There were many issue depending on using the lambda layer and in this waa it works\n",
    "    def resize_normalize(image):\n",
    "        import cv2\n",
    "        from keras.backend import tf as ktf    \n",
    "        \"\"\"\n",
    "        Applies preprocessing pipeline to an image: crops `top` and `bottom`\n",
    "        portions of image, resizes to 66*200 px and scales pixel values to [0, 1].\n",
    "        \"\"\"\n",
    "        # resize to width 200 and high 66 liek recommended\n",
    "        # in the nvidia paper for the used CNN\n",
    "        # image = cv2.resize(image, (66, 200)) #first try\n",
    "        resized = ktf.image.resize_images(image, (66, 200))\n",
    "        #normalize 0-1\n",
    "        resized = resized/255.0 - 0.5\n",
    "\n",
    "        return resized\n",
    "\n",
    "    print('I am inside call of architecture')\n",
    "    #initialize model\n",
    "    model = Sequential()\n",
    "    dropout = 0.5\n",
    "    nonlinear = 'tanh'\n",
    "    print('I am before call of cropping layer')\n",
    "    ### Convolution layers and parameters were taken from the \"nvidia paper\" on end-to-end autonomous steering.\n",
    "    model.add(Cropping2D(cropping=((60,20), (1,1)), input_shape=(160,320,3)))\n",
    "    print('I am before call of Lambda')\n",
    "    model.add(Lambda(resize_normalize, input_shape=(160, 320, 3), output_shape=(66, 200, 3)))\n",
    "    #model.add(Lambda(lambda x: resize_normalize(x), input_shape=(80,318,3), output_shape=(66, 200, 3)))\n",
    "    model.add(Convolution2D(24, 5, 5, name='conv1', subsample=(2, 2), activation=nonlinear))\n",
    "    model.add(Convolution2D(36, 5, 5, name='conv2', subsample=(2, 2), activation=nonlinear))\n",
    "    model.add(Convolution2D(48, 5, 5, name='conv3', subsample=(2, 2), activation=nonlinear))\n",
    "    model.add(Convolution2D(64, 3, 3, name='conv4', activation=nonlinear))\n",
    "    model.add(Convolution2D(64, 3, 3, name='conv5', activation=nonlinear))\n",
    "\n",
    "    ### Regression\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1164, name='hidden1', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(100, name='hidden2', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50, name='hidden3', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(10, name='hidden4', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, name='output', activation=nonlinear))    \n",
    "    \n",
    "    #model.compile(optimizer=optimizers.Adam(lr=1e-04), loss='mean_squared_error')\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    print('I am finished build the model')\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LambdaCallback, Callback\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def save_model(name):\n",
    "    # svae model to json to be ready for transfer learning\n",
    "    with open(name + '.json', 'w') as output:\n",
    "        output.write(model.to_json())\n",
    "    # Save weights and architecture    \n",
    "    model.save(name + '.h5')\n",
    "    print('I saved the model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# Training\n",
    "# ================================================================================================================\n",
    "val_best = 999\n",
    "# preload e weights if u train the model after append more traing data makes training faster.\n",
    "#model = load_model('model.h5')\n",
    "# defien model as the defined archtitecture of nvidia cnn which shoould be used for training\n",
    "model = architecture()\n",
    "num_epochs= 7\n",
    "batch_size = 128\n",
    "#define the input data for the model.fit.generator\n",
    "print(\" number of training samples: {}:\".format(len(train_samples)))\n",
    "samples_per_epoch = len(train_samples) - (len(train_samples) % batch_size)\n",
    "print('samples_per_epoch',samples_per_epoch)\n",
    "print(\" number of validation samples: {}:\".format(len(validation_samples)))\n",
    "nb_val_samples=len(validation_samples) - (len(validation_samples) % batch_size)\n",
    "print('nb_val_epoch',nb_val_samples)\n",
    "print('number of epochs:', num_epochs)\n",
    "print('I am before call of model.fit generator')\n",
    "# training pipeline with keras using a seld defined fucnction call of generator_sample\n",
    "history = model.fit_generator(generate_samples(train_samples),\n",
    "        samples_per_epoch=samples_per_epoch,\n",
    "        nb_epoch=num_epochs,\n",
    "        validation_data=generate_samples(validation_samples, augment=False),\n",
    "        nb_val_samples=nb_val_samples\n",
    "        )\n",
    "#conditioned save mdoel routine just when loss is better than before\n",
    "val_loss = history.history['val_loss'][0]\n",
    "if val_loss < val_best:\n",
    "    val_best = val_loss\n",
    "    save_model(\"model\")\n",
    "\n",
    "print('Model fit generator finished')\n",
    "print(history.history.keys()) # print out hte key from the history dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# Evaluation of the trainig results\n",
    "# ================================================================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.image as mpimg\n",
    "## plot the training and validation loss for each epoch\n",
    "print('I am ready to plot the evaluation')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()\n",
    "#plt.show(block=True)\n",
    "\n",
    "# just to end up the session--> there were some problems. the two line does not matter\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "print('===========================================================')\n",
    "print('traing session has finished')\n",
    "print('===========================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
