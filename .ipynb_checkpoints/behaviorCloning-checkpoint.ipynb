{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#common import\n",
    "import random\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "#from resize_nomalize import resize_normalize\n",
    "from generate_samples import generate_samples\n",
    "from generator_fernando import generator_fernando\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# Read in rough balanced data Set\n",
    "# ================================================================================================================\n",
    "samples = []\n",
    "with open('../data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for sample in reader:\n",
    "      samples.append(sample)\n",
    "del(samples[0])\n",
    "\n",
    "print(\" shape of the first row of samples after imread: {}:\".format(samples[0]))\n",
    "\n",
    "# Split data into training and validation set\n",
    "#sklearn.model_selection.StratifiedShuffleSplit\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "print(\" shape of the training_samples: {}:\".format(train_samples[0]))\n",
    "print(\" shape of the validation_samples: {}:\".format(validation_samples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# Model Architectures\n",
    "# The Nvidia architecture like described im here https://arxiv.org/pdf/1604.07316.pdf\n",
    "# ================================================================================================================\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers import Dense, Activation, Dropout, MaxPooling2D, Flatten, Lambda, ELU\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import models, optimizers, backend\n",
    "print('I am before call of architecture')\n",
    "\n",
    "def architecture():\n",
    "    def resize_normalize(image):\n",
    "        import cv2\n",
    "        from keras.backend import tf as ktf    \n",
    "        \"\"\"\n",
    "        Applies preprocessing pipeline to an image: crops `top` and `bottom`\n",
    "        portions of image, resizes to 66*200 px and scales pixel values to [0, 1].\n",
    "        \"\"\"\n",
    "        # resize\n",
    "        #image = cv2.resize(image, (66, 200)) #first try\n",
    "        resized = ktf.image.resize_images(image, (66, 200))\n",
    "        #normalize\n",
    "        resized = resized/255.0 - 0.5\n",
    "    \n",
    "        return resized\n",
    "\n",
    "    print('I am inside call of architecture')\n",
    "    #initialize model\n",
    "    model = Sequential()\n",
    "    dropout = 0.5\n",
    "    nonlinear = 'tanh'\n",
    "    #shifting = True\n",
    "    ### Randomly shift up and down while preprocessing\n",
    "    #shift_delta = 8 if shifting else 0\n",
    "    print('I am before call of cropping layer')\n",
    "    ### Convolution layers and parameters were taken from the \"nvidia paper\" on end-to-end autonomous steering.\n",
    "    #model.add(Cropping2D(cropping=(((random.uniform(60 - shift_delta , 60 + shift_delta)),(random.uniform(20 - shift_delta , 20 + shift_delta))), (1,1)), input_shape=(160,320,3)))\n",
    "\n",
    "    model.add(Cropping2D(cropping=((60,20), (1,1)), input_shape=(160,320,3)))\n",
    "\n",
    "    print('I am before call of Lambda')\n",
    "    model.add(Lambda(resize_normalize, input_shape=(160, 320, 3), output_shape=(66, 200, 3)))\n",
    "    #model.add(Lambda(lambda x: resize_normalize(x), input_shape=(80,318,3), output_shape=(66, 200, 3)))\n",
    "    model.add(Convolution2D(24, 5, 5, name='conv1', subsample=(2, 2), activation=nonlinear))\n",
    "    model.add(Convolution2D(36, 5, 5, name='conv2', subsample=(2, 2), activation=nonlinear))\n",
    "    model.add(Convolution2D(48, 5, 5, name='conv3', subsample=(2, 2), activation=nonlinear))\n",
    "    model.add(Convolution2D(64, 3, 3, name='conv4', activation=nonlinear))\n",
    "    model.add(Convolution2D(64, 3, 3, name='conv5', activation=nonlinear))\n",
    "\n",
    "    ### Regression\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1164, name='hidden1', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(100, name='hidden2', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(50, name='hidden3', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(10, name='hidden4', activation=nonlinear))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, name='output', activation=nonlinear))    \n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    print('I am finished build the model')\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Save Model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LambdaCallback, Callback\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def save_model(name):\n",
    "    \n",
    "    with open(name + '.json', 'w') as output:\n",
    "        output.write(model.to_json())\n",
    "\n",
    "    model.save(name + '.h5')\n",
    "    print('I saved the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# Training\n",
    "# ================================================================================================================\n",
    "numTimes = 1\n",
    "val_best = 999\n",
    "model = architecture()\n",
    "num_epochs= 3\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "#define the input data for the model.fit.generator\n",
    "print(\" number of training samples: {}:\".format(len(train_samples)))\n",
    "samples_per_epoch = len(train_samples) - (len(train_samples) % batch_size)\n",
    "print('samples_per_epoch',samples_per_epoch)\n",
    "print(\" number of validation samples: {}:\".format(len(validation_samples)))\n",
    "nb_val_samples=len(validation_samples) - (len(validation_samples) % batch_size)\n",
    "print('nb_val_epoch',nb_val_samples)\n",
    "print('number of epochs:', num_epochs)\n",
    "print('I am before call of model.fit generator')\n",
    "# training pipeline with keras\n",
    "history = model.fit_generator(#generator_fernando(X_train),\n",
    "        generate_samples(train_samples),\n",
    "        samples_per_epoch=samples_per_epoch,\n",
    "        nb_epoch=num_epochs,\n",
    "        validation_data=generate_samples(validation_samples, augment=False),\n",
    "        #validation_data=generator_fernando(y_valid),\n",
    "        nb_val_samples=nb_val_samples\n",
    "        )\n",
    "\n",
    "save_model(\"model\")\n",
    "print('Model fit generator finished')\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================================================\n",
    "# Evaluation of the trainig results\n",
    "# ================================================================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "## plot the training and validation loss for each epoch\n",
    "print('I am ready to plot the evaluation')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show(block=True)\n",
    "val_loss = history.history['val_loss'][0]\n",
    "if val_loss < val_best:\n",
    "    val_best = val_loss\n",
    "    save_model(\"model\")\n",
    "    \n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('===========================================================')\n",
    "print('traing session has finished')\n",
    "print('===========================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
